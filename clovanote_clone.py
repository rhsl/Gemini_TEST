# Streamlit + OpenAI STT(Speech-To-Text) API + GPT APIë¥¼ ì´ìš©í•œ ë„¤ì´ë²„ í´ë¡œë°”ë…¸íŠ¸ í´ë¡  êµ¬í˜„

import streamlit as st
import os
from openai import OpenAI
from pydub import AudioSegment
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import ChatPromptTemplate

def split_wav(file_path, split_interval=480000):
    # íŒŒì¼ í™•ì¥ìë¥¼ í¬í•¨í•œ ì „ì²´ íŒŒì¼ëª…ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
    file_name = os.path.splitext(os.path.basename(file_path))[0]
    
    # WAV íŒŒì¼ ë¡œë“œ
    audio = AudioSegment.from_wav(file_path)
    
    # íŒŒì¼ ê¸¸ì´(ë°€ë¦¬ì´ˆ) ê³„ì‚°
    total_duration = len(audio)
    
    part_file_name_list = []
    # 8ë¶„ ê°„ê²©ìœ¼ë¡œ íŒŒì¼ ë¶„í• 
    for i in range(0, total_duration, split_interval):
        # ë¶„í• í•  ì˜¤ë””ì˜¤ ë¶€ë¶„ ì¶”ì¶œ
        part = audio[i:i+split_interval]
        
        # ë¶€ë¶„ íŒŒì¼ ì´ë¦„ êµ¬ì„±
        part_file_name = f"{file_name}_part{i//split_interval + 1}.wav"
        part_file_name_list.append(part_file_name)        
        
        # ë¶€ë¶„ íŒŒì¼ ì €ì¥
        part.export(part_file_name, format="wav")
        print(f"Exported {part_file_name}")

    return part_file_name_list


with st.sidebar:
    openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")
    "[ì „ë¬¸ë¶„ì•¼ ì‹¬ì¸µì¸í„°ë·° ë°ì´í„° ë§í¬](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71481)"
    uploaded_file = st.file_uploader("Upload an wav file", type=("wav"))
    stt_process = st.button(":red[í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ê¸°]")
    summarization_process = st.button(":blue[í…ìŠ¤íŠ¸ ìš”ì•½í•˜ê¸°]")

st.title("ğŸ’¬ ë„¤ì´ë²„ í´ë¡œë°”ë…¸íŠ¸ í´ë¡  AI ì–´í”Œë¦¬ì¼€ì´ì…˜")
st.caption("ğŸš€ ClovaNote Clone by Streamlit + OpenAI STT(Speech-To-Text) API + OpenAI API")
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "wav íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ê¸°ë¥¼ í´ë¦­í•´ì£¼ì„¸ìš”."}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if stt_process:
    if not openai_api_key:
        st.info("Please add your OpenAI API key to continue.")
        st.stop()

    # OpenAI client ì„¤ì •
    client = OpenAI(api_key=openai_api_key)

    # wav íŒŒì¼ ë¶„í•  ì²˜ë¦¬ (25MB ì œí•œì„ ìš°íšŒí•˜ê¸° ìœ„í•´ 8ë¶„ì„ ë„˜ì–´ê°€ëŠ” wav íŒŒì¼ì„ 8ë¶„ì”© ë‚˜ëˆ ì„œ ì €ì¥)
    file_name = uploaded_file.name
    with open(file_name, "wb") as file:
        file.write(uploaded_file.getvalue())
    part_file_name_list = split_wav(file_name)

    # ë¶„í• í•œ wav íŒŒì¼ë“¤ ì½ì–´ì˜¤ê³  OpneAI STT APIë¡œ wav íŒŒì¼ ì•ˆì— í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ê¸°
    full_text = ''
    for part_file_name in part_file_name_list:
        audio_file= open(part_file_name, "rb")
        part_transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file
        )
        part_text = part_transcript.text
        full_text = full_text + part_text 
    
    st.session_state.messages.append({"role": "assistant", "content": full_text})
    st.chat_message("assistant").write(full_text)

if summarization_process:
    if not openai_api_key:
        st.info("Please add your OpenAI API key to continue.")
        st.stop()

    # ê¸´ contextë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ gpt-3.5-turbo-16kë¥¼ ì´ìš©í•´ì„œ LLM ì„¤ì •
    llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k", temperature=0, openai_api_key=openai_api_key)

    template = """ë‹¹ì‹ ì€ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤. \
    ë‚´ìš©ì˜ ì£¼ìš” ì£¼ì œì™€ ë‹¨ë½ë³„ ìš”ì•½ì„ ì§„í–‰í•˜ì‹­ì‹œì˜¤."""
    human_template = "{text}"

    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", template),
        ("human", human_template),
    ])
    chain = chat_prompt | llm

    summarization_result = chain.invoke({"text": st.session_state.messages[-1]['content']})
    st.chat_message("assistant").write(summarization_result.content)